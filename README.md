# QSRA_september_2021

1.	Outline my academic journey so far.
2.	Show you what a social network topology looks like. (Video)
3.	Describe the technologies that make up the Historical Data Digital Toolkit (HDDT).
4.	Show you how I run exercises to address my project needs using the HDDT.
5.	Stop at 1145 so that we can discuss.

# My project:
Subject: My own research, in collaboration with others, has revealed extensive social connectivity between roughly 600 Quakers, and their involvement within a community of roughly 3000 which the Quakers helped to set up and staff, who formed the four organisations in Britain active between 1830 and 1870 which I call the ‘Centres for the Emergence of Discipline of Anthropology in Britain’ (CEDA).
Question 1: What can be revealed if a historian uses data science to study a large historical community over a long period of time by bringing together and integrating metadata from catalogues, indexes, and genealogical data into a topology?
 Methodology: I have designed, built and I am now using a suite of open-source and reproducible relational database technologies and digital analytic tools to visualise and scrutinise an entire community of some 3000 activists over 40 years (1830-1870), picking out the Quakers amongst them so that the community can be explored at both group and individual levels. I model the ‘connected’ relationships between the individual members of the CEDA through time, including kinship, education, occupations, locations and organisations using topological technology.
 Question 2: What is the extent of Quaker involvement in the CEDA, over a 40-year time span and was Quaker kinship as socially cohesive as (say) education or occupation was amongst the wider community?

# 1.	Outline my academic journey so far. 
In 2016 I signed up with University of Birmingham for a part time research PhD and my interest then was in the role that Quakers in Britain played in addressing the plight of aborigines throughout the British colonies, a concern that Quakers took up in the mid-1830s.
I discovered that I would have to work across several substantial document collections to explore this theme further. The Royal Anthropological Institute, the Quaker libraries at Friends House, London and Woodbrooke in Birmingham, The Wellcome Institute, the Bodleian – all held very large document and microfilm collections that were relevant to my project.
When visiting the RAI archives in London I discovered that there was also an opportunity to explore the history of the learned societies that from 1830 to 1870, contributed to institution building in anthropology and the formation of the RAI in 1871. When I explored this new theme further with RAI archivist Sarah Walpole, I found that both themes (the plight of aborigines and institution building) had a common source in the Aborigines Protection Society, and I knew from my own research that Quakers had played an originating role, both within the APS and in parallel by supporting the Aborigines Select Committee of 1834 -37, and in the work of the Quaker Committee on the Aborigines (also in parallel).
By 2017 I had chosen to explore the Quakers and aborigines’ theme as my project, and I then spent the next two years collecting and assessing data. I performed my own research into the subscribers to the APS by reading through 8 large reels of microfilm at RAI archives. I then examined the Quaker London Yearly Meeting records at Friends House and finally the Thomas Hodgkin Collection at the Wellcome Institute.  
In 2018 I met Professor Zoe Laidlaw. Zoe and I have a shared academic interest and we met in Oxford and then London to discuss my work so far. I learned that before my PhD would complete Zoe would publish new work that would substantially cover the theme of my chosen interest – the role of Quakers in the APS. But, we also talked in general terms about how helpful it would be to a group of historians around the world if someone could build a tool kit to enable historians of colonialism to work together openly and freely and to share data. These discussions with Zoe took place before the history of colonialism reached the prominence that it now has.
Re-thinking my project, I explored if I could both take up the other theme - the role of Quakers in institution building in anthropology in Britain - and build an open-source digital toolkit that could be shared widely with others. Returning to the RAI archives to explore what data might be available I was able to start with a dataset of social data, kindly offered by Sarah Walpole (RAI archivist) on about 3000 people who were members of the RAI’s foundation societies. I also began imagining what an HDDT might be and what it might do if I could build one.
 2019 I sought the views of the Institute of Historical Research. Martin Steer and Jonathan Blaney listened patiently to my proposal and then helped me to focus it and encouraged me to pursue both building the HDDT and to use it to explore and hypothesise about the role of Quakers in institution building. I was urged to try and find technical help on UoB campus.
Aware that my IT skills were almost non-existent I began to attend one day and online courses at UoB aimed at equipping students with skills in data collection, data management and data analysis. After an arranged meeting with the UoB IT team to discuss my project, I secured the help of a dedicated Research Software Engineer attached to the College of Arts and Law, Mike Allaway. Mike offered a formal agreement to provide RSE support, one-to-one, initially for a short period after which it was extended to cover the whole of 2020 and the first half of 2021 with roughly 3 hourly sessions each fortnight with gaps so that I could work on my own and consolidate my learning. None of what I hope to show you today would have been possible without Mike’s support and engagement.
I now had a database of over 3000 names. In 2020 I met up over the phone with Ben Beck of the Quaker Family History Society and Ben kindly offered to carefully work through a list of the names (those from the RAI archive database combined with those from my own previous research) using his genealogical database to identify the Quakers amongst them. Ben found nearly 600 potential Quakers. Later he undertook the huge task of identifying the family relationships between them, for which I am truly appreciative.
I’d like to thank all those who have helped me in my work, and I’d like to acknowledge that Sarah Walpole and Ben Beck have freely given data to this project without having any idea what the HDDT might be or what I might do with it. The data you will see today is therefore not available outside of this project and I take full responsibility for the care and use of the data. If other historians choose to use the HDDT then they are welcome to do so, it is free and universally available – but they must provide their own data.
So here I am about to start year six. I have the data I need, and I have the HDDT to explore and visualise it. Zoe’s book ‘Protecting the Empire's Humanity: Thomas Hodgkin and British Colonial Activism 1830–1870’, (ZOE LAIDLAW - 2021 - CAMBRIDGE University Press) is due out at the end of this month.
Please do stretch and yawn while I set up a demonstration to show you what the HDDT can do. You will find it best to view this next part full screen if you are able to.

# 2.	Show you what a social network topology looks like. 
600 Quakers - Gephi demonstration Video link https://youtu.be/px9bieNiEac 

# 3.	Describe the technologies that make up the Historical Data Digital Toolkit (HDDT). 
Now that you have had a brief look at how Gephi can be used to visualise data for a social network spanning 40 years, I’d like to tell you how and why we built the HDDT. When I sat with Martin Steer and Jonathan Blaney at IHR some time ago I shared the data that I had then acquired, and we discussed what could be done with it. We agreed that a good suite of technologies that would work together to build an HDDT might be an SQLite database, Jupyter Notebooks and Gephi. You have already seen Gephi and the final section of today will be a quick look at a Jupyter Notebook.
I thought it important that if I built the HDDT then it must be built using popular open-source technologies that could interface well with large archive collections and keep pace with the advances being made in digitisation at national archive level. Many of us on Monday saw the impressive advances being made at Haverford and Swarthmore colleges in Pennsylvania. It was also important that the HDDT empower the individual historian, many of whom might be looking for new insights into data and who might find universal catalogue search engines restrictive. The HDDT resides ‘in the cloud’ where future archival data will no doubt also be stored and made available, and it can be easily docked to any archive capable of transmitting CSV sheets or data that can be easily rendered as CSV.
But why data in the first place? Around the middle of the eighteenth-century collectors of antiquaries and books produced an abundance of private collections and in the nineteenth century these collections began to be consolidated, systematised, and catalogued to standards. It is also in the nineteenth century that population growth became noticeably exponential. I think that historians of the modern period have at their disposal today a significant historical resource in archival catalogue ordered data and other data sets that have the potential to enable a historian to study large communities more comprehensively and with powerful visualisations and analytical tools. The pace at which technology has enabled ever larger chunks of data to be processed, even on a laptop, makes digital history compelling. About two years ago this impressed me so much that I shifted my orientation from digital history to data science because I found that the data was deeply interesting. I also found studying people anonymously, simply as nodes on a graph, both challenging and rewarding. I was not interested in famous, wealthy, or exceptional people, anyone who had got their name into an archive was of equal interest to me.
I must say a few words about ‘data cleaning’. Data cleaning is not just about looking for spelling mistakes, you will find few of them in an archival catalogue. The issue rather is that as data moves from one software program to another, glitches in the data abound. For example, those of us who use excel might think that the data one sees in a cell is what it appears to be. This is not so. Excel renders data in a way that excel thinks will please the modeller. Data science prefers raw, clean data, free of all formatting. Data cleaning tasks had to be designed, used, and audited such that I never at any time overwrote data in any cell, instead I always used formulas to transform the data and then audit techniques to prove that I had done what I wished and no more. So, when the data set comprised of 20 columns of data with over 2000 rows, that meant 20 separate exercises to ‘clean’ the columns one at a time. Then later when data moved from SQLite to Python and Python to Gephi even more data cleaning exercises had to be performed. Overall, I would say that nearly 40% of my time has been spent cleaning data.
Mike Allaway, RSE at UoB then worked with me to (1) develop an Entity Relationship Diagram to design and build the SQLite database. We worked together over many months and usually the conversation would begin with me asking ‘how?’ and Mike asking ‘why?’ Mike did most of the grunt work building the database in SQLite using VSC with me learning from him as we went along. Then I chose DBeaver as my preferred data management and data extraction tool. Now I was doing the grunt work and Mike was advising and helping with problem solving. I learned how to frame a ‘Select Query’ in the standard form SELECT (this and this column of data) FROM (this and this data table) WHERE (this and this condition is met). Soon I had captured select queries as VIEWS so that I could generate csv sheets to pass data from the SQLite database to a Python Notebook.
It soon became evident that I had a lot of work in lots of different files, and I risked losing control of the data. Mike trained me in GitHub for version control and I learned that the best structure for data management for me was to first make a GitHub repository for each task before I started work, clone the repo to my own project area on UoB OneDrive and then place there all CSV files, a python Notebook, all Gephi files, PNG files etc. I regard the structure as placing all resources in one logical container. The point here is that every time I made a change to any one of the resources in any container, I could allow GitHub to monitor and record the change, later allowing me to look back over previous versions if I wished. GitHub works at the most granular level, even putting one comma in a line of code would generate a Github change record. I think it would be unwise not to use GitHub for version control, project management and even as evidence of work done.
By now I had got the general hang of how to do simple stuff with 3 lines or so of code, and I found that I could teach myself Jupyter Notebooks, use Stack Overflow to solve coding problems and only occasionally appeal to Mike for help. Using Jupyter Notebooks I could do basic data analytics using pandas to make tables and Pyplot to make charts. I will show you a Jupyter Notebook in a few minutes. Encouraged by Martin Steer I then taught myself how to use NetworkX in Jupyter Notebooks to generate GexF files for Gephi.
Using Gephi I could do three things with powerful and insightful visualisations. (1) I could look at social networks, and we have just looked at the family connections between the Quakers present in the Centres for the Emergence of the discipline of Anthropology in Britain 1830-1870. I could also (2) use bipartite graphs to examine the way in which the individuals amongst the 3000 had the opportunity for relationships though the organisations, interests, and places that they shared. You will see a bigraph analysis in a few minutes. Lastly, I could combine the two methods and look at the community dynamically in each of the 40 years studied.
To conclude this section and in appraising what I have done with the HDDT I have found a way to powerfully visualise, and study ordered data (from catalogues, indexes and datasets) that I think calls for a different approach to the construction of nineteenth century history for digital historians, each of whom now has the capability to make unique insights that take data feeds from large archives (where both historian and archivist will soon work together in the cloud). Historians can thus make a kind of ‘living history’ where historical projects made today can later be automatically updated as new data arises. A history project can now grow and develop over time and work does not necessarily become obsolete or outdated over time, rather it might become richer and deeper. 

# 4.	Show you how I run exercises to address my project using the HDDT
3000 Institution builders - Gephi demonstration and Python Notebook 
See Example
